# Deep Reinforcement Learning Agents

Saved Model
total_steps 500 mean reward 2.8 epsilon 1
total_steps 1000 mean reward 1.5 epsilon 1
total_steps 1500 mean reward 3.3 epsilon 1
total_steps 2000 mean reward 2.1 epsilon 1
total_steps 2500 mean reward 0.8 epsilon 1
total_steps 3000 mean reward 1.6 epsilon 1
total_steps 3500 mean reward 1.2 epsilon 1
total_steps 4000 mean reward 1.7 epsilon 1
total_steps 4500 mean reward 1.6 epsilon 1
total_steps 5000 mean reward 1.3 epsilon 1
total_steps 5500 mean reward 2.5 epsilon 1
total_steps 6000 mean reward 1.0 epsilon 1
total_steps 6500 mean reward 0.8 epsilon 1
total_steps 7000 mean reward 1.6 epsilon 1
total_steps 7500 mean reward 3.8 epsilon 1
total_steps 8000 mean reward 2.9 epsilon 1
total_steps 8500 mean reward 1.8 epsilon 1
total_steps 9000 mean reward 2.2 epsilon 1
total_steps 9500 mean reward 2.3 epsilon 1
total_steps 10000 mean reward 2.7 epsilon 1
total_steps 10500 mean reward 2.7 epsilon 0.9549999999999828
total_steps 11000 mean reward 1.8 epsilon 0.9099999999999655
total_steps 11500 mean reward 2.2 epsilon 0.8649999999999483
total_steps 12000 mean reward 1.6 epsilon 0.819999999999931
total_steps 12500 mean reward 1.7 epsilon 0.7749999999999138
total_steps 13000 mean reward 2.0 epsilon 0.7299999999998965
total_steps 13500 mean reward 2.5 epsilon 0.6849999999998793
total_steps 14000 mean reward 1.1 epsilon 0.639999999999862
total_steps 14500 mean reward 1.9 epsilon 0.5949999999998448
total_steps 15000 mean reward 2.0 epsilon 0.5499999999998275
total_steps 15500 mean reward 4.0 epsilon 0.5049999999998103
total_steps 16000 mean reward 2.1 epsilon 0.4599999999998177
total_steps 16500 mean reward 2.4 epsilon 0.41499999999982823
total_steps 17000 mean reward 2.0 epsilon 0.36999999999983874
total_steps 17500 mean reward 1.5 epsilon 0.32499999999984924
total_steps 18000 mean reward 2.1 epsilon 0.27999999999985975
total_steps 18500 mean reward 2.4 epsilon 0.23499999999986562
total_steps 19000 mean reward 1.3 epsilon 0.18999999999986225
total_steps 19500 mean reward 1.7 epsilon 0.14499999999985888
total_steps 20000 mean reward 3.4 epsilon 0.09999999999985551
total_steps 20500 mean reward 1.4 epsilon 0.09999999999985551
total_steps 21000 mean reward 1.9 epsilon 0.09999999999985551
total_steps 21500 mean reward 1.4 epsilon 0.09999999999985551
total_steps 22000 mean reward 2.3 epsilon 0.09999999999985551
total_steps 22500 mean reward 2.8 epsilon 0.09999999999985551
total_steps 23000 mean reward 2.2 epsilon 0.09999999999985551
total_steps 23500 mean reward 2.5 epsilon 0.09999999999985551
total_steps 24000 mean reward 2.6 epsilon 0.09999999999985551
total_steps 24500 mean reward 3.4 epsilon 0.09999999999985551
total_steps 25000 mean reward 4.7 epsilon 0.09999999999985551
total_steps 25500 mean reward 3.4 epsilon 0.09999999999985551
total_steps 26000 mean reward 4.0 epsilon 0.09999999999985551
total_steps 26500 mean reward 2.9 epsilon 0.09999999999985551
total_steps 27000 mean reward 4.9 epsilon 0.09999999999985551
total_steps 27500 mean reward 4.3 epsilon 0.09999999999985551
total_steps 28000 mean reward 3.8 epsilon 0.09999999999985551
total_steps 28500 mean reward 4.5 epsilon 0.09999999999985551
total_steps 29000 mean reward 5.8 epsilon 0.09999999999985551
total_steps 29500 mean reward 5.0 epsilon 0.09999999999985551
total_steps 30000 mean reward 4.1 epsilon 0.09999999999985551
total_steps 30500 mean reward 7.3 epsilon 0.09999999999985551
total_steps 31000 mean reward 6.2 epsilon 0.09999999999985551
total_steps 31500 mean reward 10.4 epsilon 0.09999999999985551
total_steps 32000 mean reward 7.5 epsilon 0.09999999999985551
total_steps 32500 mean reward 7.5 epsilon 0.09999999999985551
total_steps 33000 mean reward 10.6 epsilon 0.09999999999985551
total_steps 33500 mean reward 5.3 epsilon 0.09999999999985551
total_steps 34000 mean reward 11.5 epsilon 0.09999999999985551
total_steps 34500 mean reward 12.7 epsilon 0.09999999999985551
total_steps 35000 mean reward 6.1 epsilon 0.09999999999985551
total_steps 35500 mean reward 15.0 epsilon 0.09999999999985551
total_steps 36000 mean reward 10.0 epsilon 0.09999999999985551
total_steps 36500 mean reward 9.6 epsilon 0.09999999999985551
total_steps 37000 mean reward 9.1 epsilon 0.09999999999985551
total_steps 37500 mean reward 10.6 epsilon 0.09999999999985551
total_steps 38000 mean reward 10.2 epsilon 0.09999999999985551
total_steps 38500 mean reward 12.1 epsilon 0.09999999999985551
total_steps 39000 mean reward 7.2 epsilon 0.09999999999985551
total_steps 39500 mean reward 12.0 epsilon 0.09999999999985551
total_steps 40000 mean reward 13.5 epsilon 0.09999999999985551
total_steps 40500 mean reward 12.1 epsilon 0.09999999999985551
total_steps 41000 mean reward 12.4 epsilon 0.09999999999985551
total_steps 41500 mean reward 14.1 epsilon 0.09999999999985551
total_steps 42000 mean reward 12.7 epsilon 0.09999999999985551
total_steps 42500 mean reward 13.2 epsilon 0.09999999999985551
total_steps 43000 mean reward 14.8 epsilon 0.09999999999985551
total_steps 43500 mean reward 12.1 epsilon 0.09999999999985551
total_steps 44000 mean reward 12.2 epsilon 0.09999999999985551
total_steps 44500 mean reward 15.7 epsilon 0.09999999999985551
total_steps 45000 mean reward 15.3 epsilon 0.09999999999985551
total_steps 45500 mean reward 13.3 epsilon 0.09999999999985551
total_steps 46000 mean reward 13.1 epsilon 0.09999999999985551
total_steps 46500 mean reward 19.7 epsilon 0.09999999999985551
total_steps 47000 mean reward 18.2 epsilon 0.09999999999985551
total_steps 47500 mean reward 17.2 epsilon 0.09999999999985551
total_steps 48000 mean reward 18.2 epsilon 0.09999999999985551
total_steps 48500 mean reward 14.3 epsilon 0.09999999999985551
total_steps 49000 mean reward 18.8 epsilon 0.09999999999985551
total_steps 49500 mean reward 19.8 epsilon 0.09999999999985551
total_steps 50000 mean reward 18.8 epsilon 0.09999999999985551
Saved Model
total_steps 50500 mean reward 18.4 epsilon 0.09999999999985551
total_steps 51000 mean reward 19.1 epsilon 0.09999999999985551
total_steps 51500 mean reward 20.7 epsilon 0.09999999999985551
total_steps 52000 mean reward 17.4 epsilon 0.09999999999985551
total_steps 52500 mean reward 22.6 epsilon 0.09999999999985551
total_steps 53000 mean reward 19.4 epsilon 0.09999999999985551
total_steps 53500 mean reward 22.1 epsilon 0.09999999999985551
total_steps 54000 mean reward 19.9 epsilon 0.09999999999985551
total_steps 54500 mean reward 20.9 epsilon 0.09999999999985551
total_steps 55000 mean reward 17.3 epsilon 0.09999999999985551
total_steps 55500 mean reward 18.2 epsilon 0.09999999999985551
total_steps 56000 mean reward 22.2 epsilon 0.09999999999985551
total_steps 56500 mean reward 22.9 epsilon 0.09999999999985551
total_steps 57000 mean reward 18.7 epsilon 0.09999999999985551
total_steps 57500 mean reward 23.3 epsilon 0.09999999999985551
total_steps 58000 mean reward 21.8 epsilon 0.09999999999985551
total_steps 58500 mean reward 17.7 epsilon 0.09999999999985551
total_steps 59000 mean reward 19.8 epsilon 0.09999999999985551
total_steps 59500 mean reward 20.4 epsilon 0.09999999999985551
total_steps 60000 mean reward 20.0 epsilon 0.09999999999985551
total_steps 60500 mean reward 21.9 epsilon 0.09999999999985551
total_steps 61000 mean reward 20.7 epsilon 0.09999999999985551
total_steps 61500 mean reward 17.8 epsilon 0.09999999999985551


This repository contains a collection of reinforcement learning algorithms written in Tensorflow. The ipython notebook here were written to go
along with a still-underway tutorial series I have been publishing on [Medium](https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0#.4gyadb8a4).
If you are new to reinforcement learning, I recommend reading the accompanying post for each algorithm.

The repository currently contains the following algorithms:
* **Q-Table** - An implementation of Q-learning using tables to solve a stochastic environment problem.
* **Q-Network** - A neural network implementation of Q-Learning to solve the same environment as in Q-Table.
* **Simple-Policy** - An implementation of policy gradient method for stateless environments such as n-armed bandit problems.
* **Contextual-Policy** - An implementation of policy gradient method for stateful environments such as contextual bandit problems.
* **Policy-Network** - An implementation of a neural network policy-gradient agent that solves full RL problems with states and delayed rewards, and two opposite actions (ie. CartPole or Pong).
* **Vanilla-Policy** - An implementation of a neural network vanilla-policy-gradient agent that solves full RL problems with states, delayed rewards, and an arbitrary number of actions.
* **Model-Network** - An addition to the Policy-Network algorithm which includes a separate network which models the environment dynamics.
* **Double-Dueling-DQN** - An implementation of a Deep-Q Network with the Double DQN and Dueling DQN additions to improve stability and performance.
* **Deep-Recurrent-Q-Network** - An implementation of a Deep Recurrent Q-Network which can solve reinforcement learning problems involving partial observability.
* **Q-Exploration** - An implementation of DQN containing multiple action-selection strategies for exploration. Strategies include: greedy, random, e-greedy, Boltzmann, and Bayesian Dropout.
* **A3C-Doom** - An implementation of Asynchronous Advantage Actor-Critic (A3C) algorithm. It utilizes multiple agents to collectively improve a policy. This implementation can solve RL problems in 3D environments such as VizDoom challenges.
